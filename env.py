import numpy as np
import random

class TrackingEnv:
    def __init__(self, size=100):
        self.size = size
        self.reset()

    def reset(self):
        # íƒ€ê²Ÿê³¼ ì—ì´ì „íŠ¸ ìœ„ì¹˜ ì´ˆê¸°í™”
        self.agent_pos = np.array([np.random.randint(self.size), np.random.randint(self.size)])
        self.target_pos = np.array([np.random.randint(self.size), np.random.randint(self.size)])

        # íƒ€ê²Ÿì˜ ëª©í‘œ ì§€ì  ì„¤ì •
        self.target_goal = np.random.randint(0, self.size, size=2)

        return self._get_state()

    def _get_state(self):
        # ì¢Œí‘œ ê¸°ë°˜ ìƒíƒœ ë°˜í™˜
        return np.concatenate([self.agent_pos, self.target_pos])

    def step(self, action):
        # ì—ì´ì „íŠ¸ ì´ë™
        moves = {
            0: np.array([0, -1]),  # ìƒ
            1: np.array([0, 1]),   # í•˜
            2: np.array([-1, 0]),  # ì¢Œ
            3: np.array([1, 0]),   # ìš°
        }
        move = moves.get(action, np.array([0, 0]))
        self.agent_pos = np.clip(self.agent_pos + move, 0, self.size - 1)

        # ëª©í‘œ ì§€ì ì— ë„ë‹¬í•˜ë©´ ìƒˆë¡œìš´ ëª©í‘œ ì„¤ì •
        if np.linalg.norm(self.target_goal - self.target_pos) < 1:
            self.target_goal = np.random.randint(0, self.size, size=2)

        # ëª©í‘œ ë°©í–¥ + ëœë¤ í”ë“¤ë¦¼ ì´ë™
        direction = self.target_goal - self.target_pos
        if np.linalg.norm(direction) != 0:
            move = np.round(direction / np.linalg.norm(direction)).astype(int)

            # ğŸ”¥ ëœë¤ noise ì¶”ê°€ (10% í™•ë¥ ë¡œ x/yë°©í–¥ ì‚´ì§ í”ë“¤ë¦¼)
            noise = np.random.choice([-1, 0, 1], size=2, p=[0.1, 0.8, 0.1])
            move = move + noise

            self.target_pos = np.clip(self.target_pos + move, 0, self.size - 1)

        # ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
        distance = np.linalg.norm(self.agent_pos - self.target_pos)
        reward = -distance / self.size

        done = False
        return self._get_state(), reward, done

